{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/GPU8/home/liutielong/anaconda3/envs/OmniQuant/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.models.opt.modeling_opt import OPTAttention, OPTDecoderLayer, OPTForCausalLM\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算平均用时: 8.555e-03\n",
      "读取平均用时: 7.377e-03\n",
      "总用时: 8.157\n",
      "平均总用时: 0.015931640625\n",
      "加速倍数1: 1883.045237219566\n",
      "加速倍数2: 6276.817457398553\n",
      "加速倍数3: 11298.271423317396\n",
      "几何平均加速倍数:5111.371164753922\n"
     ]
    }
   ],
   "source": [
    "def calculate(time_calc, time_read, runs=512, simu_time1=30, simu_time2=100, simu_time3=180):\n",
    "    time_total = time_calc + time_read\n",
    "    time_calc_avg = \"{:.3e}\".format(time_calc / runs)\n",
    "    time_read_avg = \"{:.3e}\".format(time_read / runs)\n",
    "    time_total_avg = time_total / runs\n",
    "    acc1 = simu_time1 / time_total_avg\n",
    "    acc2 = simu_time2 / time_total_avg\n",
    "    acc3 = simu_time3 / time_total_avg\n",
    "    acc_avg = pow(acc1 * acc2 * acc3, 1/3)\n",
    "    print(f\"计算平均用时: {time_calc_avg}\")\n",
    "    print(f\"读取平均用时: {time_read_avg}\")\n",
    "    print(f\"总用时: {time_total}\")\n",
    "    print(f\"平均总用时: {time_total_avg}\")\n",
    "    print(f\"加速倍数1: {acc1}\")\n",
    "    print(f\"加速倍数2: {acc2}\")\n",
    "    print(f\"加速倍数3: {acc3}\")\n",
    "    print(f\"几何平均加速倍数:{acc_avg}\")\n",
    "\n",
    "time_calc = 4.380\n",
    "time_read = 3.777\n",
    "runs = 512\n",
    "simu_time1 = 30\n",
    "simu_time2 = 100\n",
    "simu_time3 = 180\n",
    "calculate(time_calc, time_read, runs, simu_time1, simu_time2, simu_time3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor = torch.tensor([1, 5, 3, 7, 2, 8, 4])\n",
    "\n",
    "# 计算张量的最大值和最小值\n",
    "max_value = torch.max(tensor)\n",
    "min_value = torch.min(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  4.,  4.,  4.,  8.,  8.,  8.,  8.,  8.,  8., 16., 16., 16., 16.,\n",
      "        16., 16., 16., 16., 16., 16., 16., 32., 32., 32., 32., 32., 32., 32.,\n",
      "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
      "        32., 32., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64., 64.,\n",
      "        64., 64., 64., 64., 64., 64., 64.])\n"
     ]
    }
   ],
   "source": [
    "x_int = torch.arange(1,65)\n",
    "x_int = torch.log(x_int) / torch.log(torch.tensor(2))\n",
    "x_int.round_() - 1 \n",
    "x_int = 2 ** x_int\n",
    "print(x_int[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "tensor([ 0,  1,  2,  4,  4,  4,  8,  8,  8,  8,  8,  8, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "ori_int = torch.arange(0, 64)\n",
    "print(ori_int)\n",
    "ori_int = torch.log(ori_int) / torch.log(torch.tensor(2))\n",
    "ori_int.round_()\n",
    "ori_int = (2 ** ori_int).to(dtype=int)\n",
    "print(ori_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.log(torch.tensor(0))\n",
    "2 ** (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "class LogQuant2:\n",
    "    def __init__(self,layer,bitwidth=3):\n",
    "        self.layer_data = layer\n",
    "        self.width = bitwidth\n",
    "        self.maxima = numpy.amax(layer)\n",
    "        self.minima = numpy.amin(layer)\n",
    "        self.scale = (0.95 * self.maxima - 0.92 * self.minima) / 2\n",
    "        self.sign = numpy.sign(layer)\n",
    "\n",
    "\n",
    "    def __clip(self, x):\n",
    "        if(x < 1 - 2**self.width):\n",
    "            return 2\n",
    "        elif(x > 0):\n",
    "            return 0\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def __round(self,x):\n",
    "        halfprecision = (2**(numpy.ceil(x)) - 2**(numpy.floor(x)))/2\n",
    "        fractional = 2**(x) - 2**(numpy.floor(x))\n",
    "        if fractional >= halfprecision:\n",
    "            return numpy.ceil(x)\n",
    "        else:\n",
    "            return numpy.floor(x)\n",
    "\n",
    "    @property\n",
    "    def log_quantized(self):\n",
    "        round = numpy.vectorize(self.__round)\n",
    "        clip = numpy.vectorize(self.__clip)\n",
    "        # numpy.log2(0) -> -infinity == float(\"-inf\") which will be used in clip method\n",
    "        return numpy.array(clip(round(numpy.log2(numpy.abs(self.layer_data / self.scale)))),dtype=numpy.int8)\n",
    "\n",
    "    @property\n",
    "    def anti_quantized(self):\n",
    "        x = numpy.power(2.0, self.log_quantized)\n",
    "        x = numpy.array(x,dtype=numpy.float32)\n",
    "        x[abs(x) == numpy.power(2.0, 2)] = 0\n",
    "        return x * self.sign * self.scale\n",
    "\n",
    "numpy.random.seed(10)\n",
    "layer = numpy.random.randn(20, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.3315865   0.71527897 -1.54540029 -0.00838385  0.62133597 -0.72008556\n",
      "   0.26551159  0.10854853  0.00429143 -0.17460021  0.43302619  1.20303737\n",
      "  -0.96506567  1.02827408  0.22863013  0.44513761 -1.13660221  0.13513688\n",
      "   1.484537   -1.07980489]\n",
      " [-1.97772828 -1.7433723   0.26607016  2.38496733  1.12369125  1.67262221\n",
      "   0.09914922  1.39799638 -0.27124799  0.61320418 -0.26731719 -0.54930901\n",
      "   0.1327083  -0.47614201  1.30847308  0.19501328  0.40020999 -0.33763234\n",
      "   1.25647226 -0.7319695 ]\n",
      " [ 0.66023155 -0.35087189 -0.93943336 -0.48933722 -0.80459114 -0.21269764\n",
      "  -0.33914025  0.31216994  0.56515267 -0.14742026 -0.02590534  0.2890942\n",
      "  -0.53987907  0.70816002  0.84222474  0.2035808   2.39470366  0.91745894\n",
      "  -0.11227247 -0.36218045]\n",
      " [-0.23218226 -0.5017289   1.12878515 -0.69781003 -0.08112218 -0.52929608\n",
      "   1.04618286 -1.41855603 -0.36249918 -0.12190569  0.31935642  0.4609029\n",
      "  -0.21578989  0.98907246  0.31475378  2.46765106 -1.50832149  0.62060066\n",
      "  -1.04513254 -0.79800882]\n",
      " [ 1.98508459  1.74481415 -1.85618548 -0.2227737  -0.06584785 -2.13171211\n",
      "  -0.04883051  0.39334122  0.21726515 -1.99439377  1.10770823  0.24454398\n",
      "  -0.06191203 -0.75389296  0.71195902  0.91826915 -0.48209314  0.08958761\n",
      "   0.82699862 -1.95451212]\n",
      " [ 0.11747566 -1.90745689 -0.92290926  0.46975143 -0.14436676 -0.40013835\n",
      "  -0.29598385  0.84820861  0.70683045 -0.78726893  0.29294072 -0.47080725\n",
      "   2.40432561 -0.73935674 -0.31282876 -0.34888192 -0.43902624  0.14110417\n",
      "   0.27304932 -1.61857075]\n",
      " [-0.57311336 -1.32044755  1.23620533  2.46532508  1.38323223  0.34623312\n",
      "   1.02251611  0.16681027  1.65671662  0.66788961 -0.22994664 -1.12955119\n",
      "  -0.6399626   0.31383052 -1.22583598 -0.22179314  1.33992631  0.02930971\n",
      "   1.98538575  1.4471656 ]\n",
      " [-0.28762941 -1.35931057 -0.04804133 -0.48078734  0.37775309  1.61440797\n",
      "  -1.12310404 -0.38872795  0.33234995  1.13497317  0.51071441  0.41429764\n",
      "   1.34454942  0.49351532 -0.23700418  0.05728515 -0.70707145  0.54666484\n",
      "   0.94250041 -2.97959677]\n",
      " [ 1.21814885 -0.05652072  0.46088845  0.66237401 -2.29510333 -1.19592931\n",
      "  -0.33310116 -0.79139077  0.27417278 -0.51490992 -1.7110712   0.61229731\n",
      "   1.10012937  0.56435253 -0.71279944 -0.26085948  0.54842807  0.60319905\n",
      "   1.00686114 -0.29442601]\n",
      " [-1.42088052 -0.67894677  0.53388481  0.7439744   2.22504964  0.11718142\n",
      "   0.24461452 -0.17729882 -0.40572953  0.78177519  0.35347761 -0.20727949\n",
      "  -1.07969738 -0.12306983 -0.39098219  1.25517373  0.94712608 -1.02231069\n",
      "   1.16716837 -0.57197681]\n",
      " [ 0.1331375   1.20274387 -1.02475297  0.16039916 -1.13047529 -1.94129968\n",
      "   0.98659782  0.02822737 -0.8223676  -1.58655176 -0.40147282  1.34217928\n",
      "   0.40246909 -0.37276142 -0.67202344  0.10581208 -1.54730539  1.34480651\n",
      "   0.50318918  1.1943506 ]\n",
      " [-0.56350567  0.85482876  0.68790473 -1.53306872  0.1489607   0.31511117\n",
      "  -0.57491766 -0.36017113  0.4361853  -0.03791788 -1.00471635  0.59145309\n",
      "   0.22781736  0.24917866  0.09576324  0.44902173 -0.28600201 -0.8627583\n",
      "  -0.74182484  1.10557514]\n",
      " [-2.01771865  0.54054071 -1.44229888 -1.60885036 -1.00656856 -0.25753362\n",
      "   0.73050745 -1.69840149  1.67407555  1.1637237  -0.13257359 -0.29024581\n",
      "  -0.95353242  0.58804059  0.06880147  1.4120643  -0.68621561  0.54794393\n",
      "  -0.03638254 -0.84701625]\n",
      " [ 1.90230404  0.27960539  0.6202545  -1.06856771 -0.72262073  0.08413956\n",
      "  -0.58445516  0.60202227  0.43836524 -0.78234319  0.19293598  0.00402491\n",
      "  -0.16407529 -1.14881226 -0.8355085   0.21045073  1.01398463 -0.97019782\n",
      "   1.21718156  0.18264738]\n",
      " [-1.26981959  0.32338993  0.88577486  0.26439469  2.31912732  0.30866767\n",
      "   0.89256403  0.0110227   0.98007413 -2.39557201 -0.85752252 -0.36427809\n",
      "   0.5039269   0.18833121  1.08522707  0.35693907  0.20732957 -0.14506454\n",
      "   0.16390425  0.82951248]\n",
      " [ 0.75529996  0.87948569 -0.2399563  -0.69505665 -0.47140008  0.34947641\n",
      "   1.61145729  0.23933927  0.48500263 -1.65860471  2.67991031 -1.10352213\n",
      "   1.81778261 -0.55604734  0.95771124  0.81522068  0.0643194  -0.16640537\n",
      "  -0.94070977 -0.0535945 ]\n",
      " [ 1.1011362   0.37212255 -0.88346099  1.15805157 -1.20365349  0.20983914\n",
      "  -0.89118692 -0.78268775  1.07973068 -0.46284089 -1.51507568 -0.53871853\n",
      "   0.75662186 -0.72939922  1.21478265  1.00828297  0.07328301  1.17373241\n",
      "   1.86295167 -1.9836365 ]\n",
      " [ 1.15668893  0.60928227 -0.32320123  0.66414561  0.39672599 -0.57611686\n",
      "   1.48821839  0.14024687  1.18831675 -0.24574539 -1.63558662  0.24479809\n",
      "  -0.23508223 -1.26082781 -1.69196793  0.61645092  1.26969128  1.39733798\n",
      "   0.53422496  0.00410204]\n",
      " [ 0.18941984  0.04477469  0.79468527 -0.49314074 -1.26158974 -0.21926026\n",
      "   0.19727222 -0.25528045  0.02580759  2.20096644  0.8843676  -1.38682253\n",
      "   0.65366231 -2.17660863  0.61894892  0.4772343   0.02609653 -0.96970567\n",
      "  -0.98372812  0.41253272]\n",
      " [-0.61430485  0.83708178 -0.62361901 -0.3631862   0.75947133 -0.033927\n",
      "  -0.16640069  0.26957242  0.37530555 -0.08391524  0.22612635 -0.11588073\n",
      "   0.41160241  0.28781043  0.50427148  0.80426736 -1.03060821 -1.35769756\n",
      "  -0.26650187 -0.1712928 ]]\n"
     ]
    }
   ],
   "source": [
    "print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglayer = LogQuant2(layer,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -2 -1  2 -2 -2 -3  2  2  2 -3 -1 -2 -1  2 -3 -1  2 -1 -1]\n",
      " [-1 -1 -3  0 -1 -1  2 -1 -3 -2 -3 -2  2 -3 -1  2 -3 -3 -1 -2]\n",
      " [-2 -3 -2 -3 -2  2 -3 -3 -2  2  2 -3 -2 -2 -2  2  0 -2  2 -3]\n",
      " [ 2 -2 -1 -2  2 -2 -1 -1 -3  2 -3 -3  2 -2 -3  0 -1 -2 -1 -2]\n",
      " [ 0 -1 -1  2  2  0  2 -3  2  0 -1  2  2 -2 -2 -2 -3  2 -2 -1]\n",
      " [ 2 -1 -2 -3  2 -3 -3 -2 -2 -2 -3 -3  0 -2 -3 -3 -3  2 -3 -1]\n",
      " [-2 -1 -1  0 -1 -3 -1  2 -1 -2  2 -1 -2 -3 -1  2 -1  2  0 -1]\n",
      " [-3 -1  2 -3 -3 -1 -1 -3 -3 -1 -2 -3 -1 -3  2  2 -2 -2 -2  0]\n",
      " [-1  2 -3 -2  0 -1 -3 -2 -3 -2 -1 -2 -1 -2 -2 -3 -2 -2 -1 -3]\n",
      " [-1 -2 -2 -2  0  2  2  2 -3 -2 -3  2 -1  2 -3 -1 -2 -1 -1 -2]\n",
      " [ 2 -1 -1  2 -1 -1 -2  2 -2 -1 -3 -1 -3 -3 -2  2 -1 -1 -2 -1]\n",
      " [-2 -2 -2 -1  2 -3 -2 -3 -3  2 -1 -2  2 -3  2 -3 -3 -2 -2 -1]\n",
      " [ 0 -2 -1 -1 -1 -3 -2 -1 -1 -1  2 -3 -2 -2  2 -1 -2 -2  2 -2]\n",
      " [-1 -3 -2 -1 -2  2 -2 -2 -3 -2  2  2  2 -1 -2  2 -1 -2 -1  2]\n",
      " [-1 -3 -2 -3  0 -3 -2  2 -2  0 -2 -3 -2  2 -1 -3  2  2  2 -2]\n",
      " [-2 -2  2 -2 -3 -3 -1  2 -3 -1  0 -1 -1 -2 -2 -2  2  2 -2  2]\n",
      " [-1 -3 -2 -1 -1  2 -2 -2 -1 -3 -1 -2 -2 -2 -1 -1  2 -1 -1  0]\n",
      " [-1 -2 -3 -2 -3 -2 -1  2 -1  2 -1  2  2 -1 -1 -2 -1 -1 -2  2]\n",
      " [ 2  2 -2 -3 -1  2  2 -3  2  0 -2 -1 -2  0 -2 -3  2 -2 -2 -3]\n",
      " [-2 -2 -2 -3 -2  2  2 -3 -3  2  2  2 -3 -3 -2 -2 -1 -1 -3  2]]\n"
     ]
    }
   ],
   "source": [
    "layer_quant = loglayer.log_quantized\n",
    "print(layer_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.32178596  0.66089298 -1.32178596 -0.          0.66089298 -0.66089298\n",
      "   0.33044649  0.          0.         -0.          0.33044649  1.32178596\n",
      "  -0.66089298  1.32178596  0.          0.33044649 -1.32178596  0.\n",
      "   1.32178596 -1.32178596]\n",
      " [-1.32178596 -1.32178596  0.33044649  2.64357191  1.32178596  1.32178596\n",
      "   0.          1.32178596 -0.33044649  0.66089298 -0.33044649 -0.66089298\n",
      "   0.         -0.33044649  1.32178596  0.          0.33044649 -0.33044649\n",
      "   1.32178596 -0.66089298]\n",
      " [ 0.66089298 -0.33044649 -0.66089298 -0.33044649 -0.66089298 -0.\n",
      "  -0.33044649  0.33044649  0.66089298 -0.         -0.          0.33044649\n",
      "  -0.66089298  0.66089298  0.66089298  0.          2.64357191  0.66089298\n",
      "  -0.         -0.33044649]\n",
      " [-0.         -0.66089298  1.32178596 -0.66089298 -0.         -0.66089298\n",
      "   1.32178596 -1.32178596 -0.33044649 -0.          0.33044649  0.33044649\n",
      "  -0.          0.66089298  0.33044649  2.64357191 -1.32178596  0.66089298\n",
      "  -1.32178596 -0.66089298]\n",
      " [ 2.64357191  1.32178596 -1.32178596 -0.         -0.         -2.64357191\n",
      "  -0.          0.33044649  0.         -2.64357191  1.32178596  0.\n",
      "  -0.         -0.66089298  0.66089298  0.66089298 -0.33044649  0.\n",
      "   0.66089298 -1.32178596]\n",
      " [ 0.         -1.32178596 -0.66089298  0.33044649 -0.         -0.33044649\n",
      "  -0.33044649  0.66089298  0.66089298 -0.66089298  0.33044649 -0.33044649\n",
      "   2.64357191 -0.66089298 -0.33044649 -0.33044649 -0.33044649  0.\n",
      "   0.33044649 -1.32178596]\n",
      " [-0.66089298 -1.32178596  1.32178596  2.64357191  1.32178596  0.33044649\n",
      "   1.32178596  0.          1.32178596  0.66089298 -0.         -1.32178596\n",
      "  -0.66089298  0.33044649 -1.32178596 -0.          1.32178596  0.\n",
      "   2.64357191  1.32178596]\n",
      " [-0.33044649 -1.32178596 -0.         -0.33044649  0.33044649  1.32178596\n",
      "  -1.32178596 -0.33044649  0.33044649  1.32178596  0.66089298  0.33044649\n",
      "   1.32178596  0.33044649 -0.          0.         -0.66089298  0.66089298\n",
      "   0.66089298 -2.64357191]\n",
      " [ 1.32178596 -0.          0.33044649  0.66089298 -2.64357191 -1.32178596\n",
      "  -0.33044649 -0.66089298  0.33044649 -0.66089298 -1.32178596  0.66089298\n",
      "   1.32178596  0.66089298 -0.66089298 -0.33044649  0.66089298  0.66089298\n",
      "   1.32178596 -0.33044649]\n",
      " [-1.32178596 -0.66089298  0.66089298  0.66089298  2.64357191  0.\n",
      "   0.         -0.         -0.33044649  0.66089298  0.33044649 -0.\n",
      "  -1.32178596 -0.         -0.33044649  1.32178596  0.66089298 -1.32178596\n",
      "   1.32178596 -0.66089298]\n",
      " [ 0.          1.32178596 -1.32178596  0.         -1.32178596 -1.32178596\n",
      "   0.66089298  0.         -0.66089298 -1.32178596 -0.33044649  1.32178596\n",
      "   0.33044649 -0.33044649 -0.66089298  0.         -1.32178596  1.32178596\n",
      "   0.66089298  1.32178596]\n",
      " [-0.66089298  0.66089298  0.66089298 -1.32178596  0.          0.33044649\n",
      "  -0.66089298 -0.33044649  0.33044649 -0.         -1.32178596  0.66089298\n",
      "   0.          0.33044649  0.          0.33044649 -0.33044649 -0.66089298\n",
      "  -0.66089298  1.32178596]\n",
      " [-2.64357191  0.66089298 -1.32178596 -1.32178596 -1.32178596 -0.33044649\n",
      "   0.66089298 -1.32178596  1.32178596  1.32178596 -0.         -0.33044649\n",
      "  -0.66089298  0.66089298  0.          1.32178596 -0.66089298  0.66089298\n",
      "  -0.         -0.66089298]\n",
      " [ 1.32178596  0.33044649  0.66089298 -1.32178596 -0.66089298  0.\n",
      "  -0.66089298  0.66089298  0.33044649 -0.66089298  0.          0.\n",
      "  -0.         -1.32178596 -0.66089298  0.          1.32178596 -0.66089298\n",
      "   1.32178596  0.        ]\n",
      " [-1.32178596  0.33044649  0.66089298  0.33044649  2.64357191  0.33044649\n",
      "   0.66089298  0.          0.66089298 -2.64357191 -0.66089298 -0.33044649\n",
      "   0.66089298  0.          1.32178596  0.33044649  0.         -0.\n",
      "   0.          0.66089298]\n",
      " [ 0.66089298  0.66089298 -0.         -0.66089298 -0.33044649  0.33044649\n",
      "   1.32178596  0.          0.33044649 -1.32178596  2.64357191 -1.32178596\n",
      "   1.32178596 -0.66089298  0.66089298  0.66089298  0.         -0.\n",
      "  -0.66089298 -0.        ]\n",
      " [ 1.32178596  0.33044649 -0.66089298  1.32178596 -1.32178596  0.\n",
      "  -0.66089298 -0.66089298  1.32178596 -0.33044649 -1.32178596 -0.66089298\n",
      "   0.66089298 -0.66089298  1.32178596  1.32178596  0.          1.32178596\n",
      "   1.32178596 -2.64357191]\n",
      " [ 1.32178596  0.66089298 -0.33044649  0.66089298  0.33044649 -0.66089298\n",
      "   1.32178596  0.          1.32178596 -0.         -1.32178596  0.\n",
      "  -0.         -1.32178596 -1.32178596  0.66089298  1.32178596  1.32178596\n",
      "   0.66089298  0.        ]\n",
      " [ 0.          0.          0.66089298 -0.33044649 -1.32178596 -0.\n",
      "   0.         -0.33044649  0.          2.64357191  0.66089298 -1.32178596\n",
      "   0.66089298 -2.64357191  0.66089298  0.33044649  0.         -0.66089298\n",
      "  -0.66089298  0.33044649]\n",
      " [-0.66089298  0.66089298 -0.66089298 -0.33044649  0.66089298 -0.\n",
      "  -0.          0.33044649  0.33044649 -0.          0.         -0.\n",
      "   0.33044649  0.33044649  0.66089298  0.66089298 -1.32178596 -1.32178596\n",
      "  -0.33044649 -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "layer_dequant = loglayer.anti_quantized\n",
    "print(layer_dequant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "class Quantizer():\n",
    "    def __init__(self, symmetric, n_bits) -> None:\n",
    "        self.n_bits = n_bits\n",
    "        if symmetric:\n",
    "            self.qmin = -2 ** (2 ** (n_bits - 1) - 1)\n",
    "            self.qmax = 2 ** (2 ** (n_bits - 1) - 2)\n",
    "        else:\n",
    "            self.qmin = 0\n",
    "            self.qmax = 2 ** (2 ** n_bits -2)\n",
    "\n",
    "q1 = Quantizer(symmetric=False, n_bits=3)\n",
    "print(q1.qmax)\n",
    "print(q1.qmin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OmniQuant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
